import os
import sys
from openai_api import *
import json
import re
from copy import deepcopy
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

import numpy as np
import argparse

# cite the implementation of StructFlowBench

base_dir=os.path.dirname(os.path.abspath(__file__))

            
 

class Evaluate_api():
    def __init__(self,in_dir,max_try):
        self.prompt_template="""
[Conversation History]
{conv_history}

[Current Round User Prompt]
{cur_user_prompt}

[Current Round Response]
{cur_response}

[Check List]
{check_list}

[Golden Response]
{golden_response}

[Task]
You are an exceedingly meticulous and fair judge. Your task is to rigorously evaluate whether the [Current Round Response] strictly adheres to every detail specified in the [Current Round User Prompt], using the provided [Check List] as your guide.
- [Conversation History] provides context from previous rounds of the dialogue.
- [Current Round User Prompt] represents the latest instruction given by the user in the dialogue; each aspect of this prompt must be addressed with exactness and thoroughness.
- [Current Round Response] is the response generated by the language model in accordance with the user's prompt.
- [Check List] contains specific questions that assess whether the [Current Round Response] meets each detailed requirement outlined about the correct response; each item must be scrutinized meticulously.
- [Golden Response] is the groundtruth response that can be referenced (Since the conversation history may be different, [Golden Response] may not fully conform to the context content, but it can refer to its satisfaction of the constraints)

For each item in the [Check List], answer with 'Yes' if the criterion is met beyond doubt, or 'No' if there is any deviation, ambiguity, or omission. Provide a clear and concise explanation for your judgment, highlighting how the response does or does not meet the criteria. Justify your answer with reference to both the [Current Round User Prompt] and relevant parts of the [Conversation History].


**Note**: Some constraints are based on the multi-round dialogue. Please refer to the multi-round dialogue when evaluating, ensuring absolute fidelity to the context and instructions given.
**Note**: Ensure that all items in [Check List] are rigorously judged, with no omissions and no allowances for partial compliance!
**Deliverable**: Provide judgement following the designated [Output Format] without including extra analysis or commentary. Any failure to adhere to these instructions should result in a 'No' assessment.

[Output Format]
```json
{{
    "result":[
        {{
            "judgement":"<str:only 'Yes' or 'No', indicating whether the constraint was followed.>",
            "reason":"<str:Provide an explanation for your judgment basis, i.e., the reasoning behind determining whether the constraint was followed>"
        }},
        ...
    ]
}}
```
**Note**:"result" means the judge result of [Current Round Response].
**Note**:Ensure that the order of the judgments in result is consistent with the order of the questions in the [Check List] and corresponds one to one
**Note**: Answer strictly according to the requirements of the items in [Check List], and do not let the reasons for your judgment be inconsistent with the requirements. For example, if the requirement is "Does the response include the keywords 'brushstroke' and 'color'?", You only need to determine whether the response includes the specified keywords, without worrying about whether the response meets the requirements of [Current Round User Prompt].
**Note**:You must correctly understand the constraints in the [Check List]. For example, the constraint "Check whether the first letter of the response is 'T'" means that the first letter of the response is 'T', if the response starts with 'The', it meets the constraint.Sometimes you will have hallucinations, such as the following judgment: "The response starts with 'The', so its first letter is not 'T'." Don't complicate the problem. As long as the first letter of the first word is 'T', it meets the requirements.
**Note**: If the check item is "Does the response not use any commas?", you only need to check whether the response contains any commas, regardless of other punctuation marks.
**Note**:[Golden Response] provides an answer reference. When judging whether [Current Round Response] satisfies the corresponding constraints, you should refer to [Golden Response] to make a more accurate and serious judgment.

Let's begin:

[Current Round Response]
{cur_response}
"""
        self.input_file=in_dir
        #self.output_file=out_dir
        self.max_try=max_try

        
        

    def _process_single_item(self,item):
        conv_data=item["conv"]
        for conv_turn_idx in range(0,len(conv_data)):
            conv_history=""
            for idx in range(0,conv_turn_idx):
                user_prompt=("user"+":"+conv_data[idx]["user"]+"\n")
                assistant_ans=("LLM assistant"+":"+conv_data[idx]['sys']+"\n")
                conv_history+=(f"Round{idx}:\n"+user_prompt+assistant_ans)
            
            cur_user_prompt=conv_data[conv_turn_idx]["user"]+"\n"
            cur_response=conv_data[conv_turn_idx]['sys']
            golden_response=conv_data[conv_turn_idx]['groundtruth']
            check_list=""
            for idx,check_item in enumerate(conv_data[conv_turn_idx]["check_list"]):
                check_list+=(f"{idx+1}:{check_item}")
            check_num=len(conv_data[conv_turn_idx]["check_list"])
            prompt = self.prompt_template.format(conv_history=conv_history,cur_user_prompt=cur_user_prompt,cur_response=cur_response,check_list=check_list,golden_response=golden_response)
            try_time = 0
            while try_time < self.max_try:
                try:
           
                    generated_text=openai_chat(prompt)
                    #print(generated_text)
                    if generated_text.startswith("```json"):
                        generated_text = generated_text[7:]
                    if generated_text.endswith("```"):
                        generated_text = generated_text[:-3]
                    generated_json = json.loads(generated_text)
                    if len(generated_json["result"])!=check_num:
                        print("retry")
                        try_time+=1
                        continue
                    conv_data[conv_turn_idx]["result"] = generated_json["result"]
                    break
                except json.JSONDecodeError as e:
                    print(f"JSON decode error: {e}, content: {generated_text}")
                    conv_data[conv_turn_idx]["judge result"] = {"error": "Invalid JSON", "content": generated_text}
                    break


    def judge_process(self,output_dir,max_workers=5):
        with open(self.input_file,'r',encoding='utf-8') as f:
            infer_result=json.load(f)
        os.makedirs(os.path.dirname(output_dir), exist_ok=True)

        with ThreadPoolExecutor(max_workers) as executor: 
            futures = [
                executor.submit(self._process_single_item, item)
                for i, item in enumerate(infer_result)
                
            ]

            for future in tqdm(as_completed(futures), total=len(futures)):
                pass
        json.dump(infer_result, open(output_dir,'w',encoding='utf-8'),ensure_ascii=False,indent=4)      
      

def new_score(data_dir):
    with open(data_dir,'r')as f:
        data=json.load(f)
    final_result=[]

    drfr_list = []
    isr_list = []
    csr_list = []
    wcsr_list = []
    specified_dict={}
    for i,row in enumerate(data):
        for j,conv in enumerate(row['conv']):
            if 'result' not in conv:
                continue
            
            agent_results=conv['result']
            
            cur_csr_results = []
            cur_isr = 1
            cur_wcsr_numerator = 0
            cur_wcsr_denominator = 0
            constraint_list=conv['constraint_type']
            for judge_result ,type in zip(agent_results,constraint_list):
                result = 1 if judge_result['judgement'] == 'Yes' else 0
                if type not in specified_dict:
                    specified_dict[type]=[]
                specified_dict[type].append(result)
                if result!=1:
                    cur_isr=0
                drfr_list.append(result)
                cur_csr_results.append(result)
                if type=='instruction':
                    weight=1
                else:
                    weight=2
                cur_wcsr_numerator+=result*weight
                cur_wcsr_denominator+=weight
            
            csr_value = np.mean(cur_csr_results) if cur_csr_results else 0
            csr_list.append(csr_value)
            isr_list.append(cur_isr)
            wcsr_value = cur_wcsr_numerator / cur_wcsr_denominator if cur_wcsr_denominator != 0 else 0
            wcsr_list.append(wcsr_value)
    
    statistics_agent_result = {
        "overall": {
            "CSR": np.mean(csr_list) if csr_list else 0,
            "ISR": np.mean(isr_list) if isr_list else 0,
            "DRFR": np.mean(drfr_list) if drfr_list else 0,
            "WCSR": np.mean(wcsr_list) if wcsr_list else 0
        }
    }          
    for type in specified_dict:
        metric=np.mean(specified_dict[type])
        statistics_agent_result[type]=metric
    print(statistics_agent_result)
    final_result.append(statistics_agent_result)


    filename_with_ext = os.path.basename(data_dir) 

    filename = os.path.splitext(filename_with_ext)[0] 
    save_dir=os.path.join(os.path.dirname(data_dir),filename+'_statistics.json')
    json.dump(final_result, open(save_dir,'w',encoding='utf-8'),ensure_ascii=False,indent=4)

def evaluate_and_score(data_dir,judge_save_dir,max_workers=5):
    evaluator=Evaluate_api(
        in_dir=data_dir,
        max_try=5,
    )
    evaluator.judge_process(judge_save_dir,max_workers=max_workers)
    new_score(judge_save_dir)

        
if __name__=="__main__":
    parser = argparse.ArgumentParser(description="StructBench Evaluation Script")
    parser.add_argument('--input_dir', type=str, required=True, help='Path to the input data file (e.g., Llama-3.1-8B-Instruct.json)')
    parser.add_argument('--output_dir', type=str, required=True, help='Path to save the evaluation results')
    parser.add_argument('--max_workers', type=int, default=8, help='Number of worker threads for evaluation')
    args = parser.parse_args()

    evaluate_and_score(args.input_dir, args.output_dir, args.max_workers)
